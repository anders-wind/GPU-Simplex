% !TeX spellcheck=en_GB
\section{Design}
This section contains an analysis of the scope for parallelisation of Simplex and our design strategy for implementing parallel Simplex.

\subsection{Scope for Parallelism in Simplex}
The bulk of the work in the Simplex algorithm happens in the convergence loop containing the pivot procedure. By its nature, the convergence loop is unparallelisable since every iteration depends on the previous iteration. However, pivot itself is highly parallel as the values in the new $A'$ matrix and $b'$ and $c'$ vectors are computed independently of each other by only using values from the previous $A$, $b$, and $c$.

Nevertheless, the execution time under a parallel pivot will still be dominated by the number of iterations in the convergence loop, which is worst-case exponential in the size of the problem. This is the limiting factor for the potential performance gains from parallelisation.

\subsection{Our Approach}
To get around the hard limit on parallelisation by the convergence loop, our approach is to solve several linear program instances in bulk. The idea is to amortise the convergence time across multiple instances. This approach is hereafter referred to as Multi-Simplex.

In order to explore the effects of parallelism, we implement three versions of the algorithm that exploit different degrees of parallelism: outer parallel across instances, inner parallel across Simplex, and fully parallel across instances and Simplex.

We base the sequential Simplex algorithm on the algorithm presented in [Introduction to Algorithms]\footnote{Third edition, Chapter 29} by Thomas H. Cormen et al. The basic Multi-Simplex algorithm along with pivot is shown in pseudocode in Figure \ref{code:multi-simplex}.

\begin{figure}[H]
\begin{verbatim}
Multi-Simplex(As[h][m][n], bs[h][m], cs[h][n]) =
  map
    (\A b c =>
      v = 0
      e = Entering-Variable(c)
      while (e != -1) do
        l = Leaving-Variable(A,b,e)
        (A,b,c,v) = Pivot(A,b,c,v,e,l)
        e = Entering-Variable(c)
      done
      return v
    )
    As bs cs

Pivot(A[m][n], b[m], c[n], v, e, l) =
  newA = map (\i => map (\j -> FuncA(A,i,j,e,l)) (iota n)) (iota m)
  newB = map (\i => FuncB(A,b,i,e,l)) (iota m)
  newC = map (\i -> FuncC(newA,c,i,e,l)) (iota n)
  newV = FuncV(newB,c,v,e)
  return (newA, newB, newC, newV)
\end{verbatim}
\caption{Pseudocode for \texttt{Multi-Simplex} and \texttt{Pivot}. Here \texttt{h} is the number of instances and an instance \texttt{i} is represented by the constraints matrix \texttt{As[i]}, constants vector \texttt{bs[i]}, and coefficients vector \texttt{cs[i]}.\newline
The inner workings of \texttt{Pivot} are abstracted out to solely show the data dependencies and parallel operations (\texttt{Funcx} functions consist of small if-statements and arithmetic operations). \texttt{Entering-Variable} and \texttt{Leaving-Variable} are not included here, but both contain a reduce and an iota.}
\label{code:multi-simplex}
\end{figure}

\subsubsection{Assumptions and Limitations}
For simplification, we assume the following about the linear program instances:
\begin{itemize}
\item They are \textbf{bounded}: that is, they have a finite objective value.
\item They are \textbf{dense}: common applications of linear programming are often sparse in the constraints matrix which make them amenable to different representations. We considered a sparse matrix representation, but it was not an entirely natural approach given the regularity of accesses in the original Simplex algorithm.
\item They have an \textbf{initial feasible basic solution}: instances are not guaranteed to begin in the feasible region and so Simplex typically has an initialisation procedure which either ensures this, or reports that the linear program is infeasible. The procedure is similar to Simplex itself, so for simplicity, we exclude it.
\end{itemize}
We do \textit{not} assume that Simplex will not cycle, or that the multiple instances are the same size.

The selection of the entering and leaving variables determine how fast the loop converges, but there is no general optimal rule. Typically the leaving variable is selected based on the entering variable, and the entering variable is selected by some heuristic. Our strategy is to always pick the non-basic variable with the smallest index as the entering variable (Bland's rule). This may not ensure the fastest convergence, but it does have the useful property that it guarantees termination of the algorithm, i.e. it avoids cycles.

\subsubsection{Outer Parallel Multi-Simplex}
In this version, the multiple instances of linear programs are solved in parallel, but each individual instance is solved sequentially. This is exactly the version shown in Figure \ref{code:multi-simplex}, where the outer map is parallel. Since the inner dimension is computed sequentially, it is not necessary to flatten the input. We did not take transposition for memory-coalesced access into account, since we need to access the matrices from both rows and columns.

For this type of parallelism to perform well, the number of instances must be near or exceed the number of threads the GPU has to offer, as a small number of instances would not properly exploit the parallelism.

\subsubsection{Inner Parallel Multi-Simplex}
In this version, the multiple instances are solved sequentially, but each individual instance is solved in parallel. In particular, this exploits the parallelism of \texttt{Pivot}. The \texttt{A} matrix is updated with a nested map during \texttt{Pivot}, and this nested map is flattened into a single map. This is the only change needed to facilitate full parallelism in \texttt{Pivot}.

This type of parallelism is potentially useful for very large instances with many constraints and variables that are updated in parallel in \texttt{Pivot}. The main bottleneck will then likely be the number of iterations needed for convergence for each instance as well as the outer sequential loop across instances.

\subsubsection{Fully Parallel Multi-Simplex}
In this version, the multiple instances are solved fully in parallel with Simplex rewritten to take in and solve several instances at once. Every previous map, iota, reduce or scan operation is now inside a parallel map and therefore flattening techniques are required to remove the nested parallelism.

Furthermore, since each instance can have different dimensions, we need a way to keep track of their position inside each flattened array of \texttt{A}, \texttt{b}, and \texttt{c}. We therefore add two parameters, shape arrays \texttt{ns} and \texttt{ms} containing the number of variables $n$ and constraints $m$ for each instance.

\newpar
Let $h$ be the number of instances, and let $A_{kij}$ denote the element on the $i$'th row and $j$'th column in the $k$'th instance in $A$. To find the element $A_{kij}$ in \texttt{A}, we need to know where instance \texttt{k} starts in \texttt{A}. Each instance \texttt{k} takes up \texttt{ns[k] * ms[k]} entries in \texttt{A}. We therefore create \texttt{segment\_indices}, an array containing the indices for which each individual instance begins in \texttt{A}.

If we want to create a new \texttt{A} using map, we need two arrays of the same length as \texttt{A} for indexing, \texttt{segment\_numbers} and \texttt{segment\_iotas}. The idea is that for some absolute index \texttt{p}, \texttt{A[p]} corresponds to $A_{kij}$ if \texttt{k = segment\_numbers[p]} and \texttt{i * ns[k] + j = segment\_iotas[p]}. The three indexing arrays can be calculated as shown in Figure \ref{code:index}.
\begin{figure}[H]
\begin{verbatim}
segment_indices = scan_exc (+) 0 (map (*) ns ms)
flags = scatter [0,0,...,0] segment_indices [1,1,...,1]
segment_numbers = map (-1) (scan_inc (+) 0 flags)
segment_iotas = segmented_scan_exc (+) 0 flags [1,1,...,1]
\end{verbatim}
\caption{Pseudocode for calculating indexing arrays.}
\label{code:index}
\end{figure}
\noindent If we map over \texttt{segment\_numbers} and \texttt{segment\_iotas}, we can get the instance number and the index into the instance number, respectively, and from that calculate \texttt{p}. Something similar can be done for \texttt{c} and \texttt{b}, but where the entries in \texttt{segment\_iotas} are just the direct indices instead of the flattened matrix indices.

\newpar
A key observation here is that the three indexing arrays stay constant across the iterations of the convergence loop. They can therefore be hoisted outside the loop and only need to be calculated once, such that the cost is amortised over the iterations in the loop.

\newpar
The original entering variable and leaving variable methods consisted of a reduce which is now computed on all instances by using segmented scan with the same operator. We had some difficulties with ensuring that the scan operator was truly associative which resulted in getting the correct result when running sequentially but the wrong result in parallel. The problem originated from the fact that the neutral element can be placed on both sides of the operator, which we had not taken into consideration and only handled it for the left side.

\newpar
As for the inner parallel version, the convergence loop cannot be parallelised. This has all the same implications, but with the added problem that the number of iterations executed now always corresponds to the most expensive instance. This implies that potentially a lot of threads will do busy work on already completed instances while one of the instances is still incomplete. This only becomes a problem if the level of parallelism exceeds the resources of the hardware though.
